#!/usr/bin/env python
# coding: utf-8

import streamlit as st
import nltk
import tempfile
import asyncio
import os
import pickle
import shutil

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import NLTKTextSplitter
from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

# -------------------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£ÙˆÙ„ÙŠØ©
# -------------------------------------------
st.set_page_config(page_title="PDF Q&A with LangChain", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ“˜ Ask your PDF using Huggingface + Gemini + LangChain")

# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK
nltk.download('punkt', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('punkt_tab', quiet=True)

# -------------------------------------------
# Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù
# -------------------------------------------
uploaded_file = st.file_uploader("ğŸ“„ Upload your PDF file", type=["pdf"])

if uploaded_file is not None:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.getbuffer())
        file_path = tmp.name

    st.success("âœ… File uploaded successfully!")

    # -------------------------------------------
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø§Øª Ù…Ù† PDF
    # -------------------------------------------
    loader = PyPDFLoader(file_path)
    pages = []

    async def load_pages():
        async for page in loader.alazy_load():
            pages.append(page)

    asyncio.run(load_pages())
    st.info(f"ğŸ“„ Loaded {len(pages)} pages from your PDF.")

    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ
    documents = [page.page_content for page in pages]
    metadata = {"document": uploaded_file.name}
    text_splitter = NLTKTextSplitter(chunk_size=400)
    chunks = text_splitter.create_documents(documents, metadatas=[metadata] * len(documents))
    st.write(f"ğŸ§© Total chunks created: {len(chunks)}")

    # -------------------------------------------
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ÙˆØ§Ù„Ù€ Embeddings
    # -------------------------------------------
    API_KEY = st.text_input("ğŸ”‘ Enter your Google API Key:", type="password")

    if API_KEY:
        embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

        chat_model = ChatGoogleGenerativeAI(
            google_api_key=API_KEY,
            model="gemini-2.5-flash"
        )

        # -------------------------------------------
        # Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ…Ù„ÙØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        # -------------------------------------------
        base_dir = os.path.join(os.getcwd(), "vector_db")
        os.makedirs(base_dir, exist_ok=True)
        pdf_name = os.path.splitext(uploaded_file.name)[0]
        pkl_path = os.path.join(base_dir, f"{pdf_name}_embeddings.pkl")
        db_dir = os.path.join(base_dir, pdf_name)

        # -------------------------------------------
        # Ù„Ùˆ ÙÙŠ mismatch ÙÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ù†Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        # -------------------------------------------
        if os.path.exists(db_dir):
            try:
                _ = Chroma(persist_directory=db_dir, embedding_function=embeddings_model)
            except Exception as e:
                if "dimension" in str(e).lower():
                    shutil.rmtree(db_dir)
                    st.warning("âš ï¸ Old database deleted due to dimension mismatch.")

        # -------------------------------------------
        # ØªØ­Ù…ÙŠÙ„ embeddings Ù…Ù† Ù…Ù„Ù pkl Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯
        # -------------------------------------------
        if os.path.exists(pkl_path):
            st.info("ğŸ“¦ Loading precomputed embeddings...")
            with open(pkl_path, "rb") as f:
                chunks = pickle.load(f)
        else:
            st.info("ğŸ§  Generating new embeddings (first time only)...")
            # Ù†Ø­ÙØ¸ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù„Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
            with open(pkl_path, "wb") as f:
                pickle.dump(chunks, f)
            st.success(f"ğŸ’¾ Embeddings saved as {pkl_path}")

        # -------------------------------------------
        # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        # -------------------------------------------
        vector_db = Chroma.from_documents(chunks, embeddings_model, persist_directory=db_dir)

        # -------------------------------------------
        # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„
        # -------------------------------------------
        question = st.text_input("â“ Ask your question:")

        if question:
            st.write("ğŸ” Searching for the most relevant context...")
            similar_docs = vector_db.similarity_search(question, k=5)

            qna_template = """
            Answer the next question using the provided context.
            If the answer is not contained in the context, say 'NO ANSWER IS AVAILABLE'

            ### Context:
            {context}

            ### Question:
            {question}

            ### Answer:
            """

            qna_prompt = PromptTemplate(
                template=qna_template,
                input_variables=["context", "question"],
            )

            prompt = ChatPromptTemplate.from_template(qna_template)
            chain = create_stuff_documents_chain(chat_model, prompt)



            with st.spinner("ğŸ¤” Generating answer..."):
                answer = chain.invoke({"context": similar_docs, "question": question})

            st.success("âœ… Answer generated:")
            st.write(answer["output_text"])



